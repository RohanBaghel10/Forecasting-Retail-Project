---
title: "Retail Project"
author: "Rohan Baghel (32725787)"
date: "2023-05-15"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(fpp3)
library(tidyverse) 
library(urca)
library(readxl)
library(readabs)
```


```{r}
set.seed(32725787)
myseries <- aus_retail |>
  # Remove discontinued series
  filter(!(`Series ID` %in% c("A3349561R","A3349883F","A3349499L","A3349902A",
                        "A3349588R","A3349763L","A3349372C","A3349450X",
                        "A3349679W","A3349378T","A3349767W","A3349451A"))) |>
  # Select a series at random
  filter(`Series ID` == sample(`Series ID`,1))
```

### The statistical features of the Data 

```{r }
myseries %>% 
  autoplot() +
  labs(title = " Other Retail Turnover for Queensland", 
           y = "Turnover (millions($AUD))")

summary(myseries$Turnover)
```
From the plot we can observe that the turnover has an increasing trend and that there is strong seasonality in this trend as well. 

```{r}
myseries %>% 
  gg_subseries(Turnover) +
  labs(title = "Seasonal Subseries Plot for Other Retail Turnover for Queensland", 
           y = "Turnover (millions($AUD))")
```

This plot shows the underlying seasonal pattern for the turnover in the months of a calendar year. The blue horizontal lines indicate the mean turnover for each month. We can observe the increase in retail for December due to holiday season.


### Transformations Used

```{r}
myseries %>% 
  autoplot(box_cox(Turnover,
                   lambda = 0.1)) +
  labs(title = "The Box Cox Transformation for Retain Turnover in Queensland")

#Finding The lambda through guerrero's method
myseries %>% 
  features(Turnover, features = guerrero)
```
The box cox transformation for retail data in queensland given above has a **lambda = 0.1** . The lambda of 0.1 was selected because it gave a constant variation compared to other values. The seasonality and trend remain similar to that of the original data. It gives a clear picture of the features of seasonality, trend and variation. 

After using the **guerrero's method** , the lambda was found to be **0.1119099**. Thus there is not a significant change when considering other values for the lambda when considering the initial value of **0.1**.

```{r}
#STL Decomposition of Data 

STL_myseries <- myseries %>%
  model(stl = STL(box_cox(Turnover, 0.1) ~ trend(window = 51) + season(window = 20)))


STL_myseries %>% 
  components() %>% 
  autoplot() +
  labs(title = "STL decomposition of for Retain Turnover in Queensland ")

```

The STL-decomposition graph reflects the components of the original plot  in three different forms. The Trend, seasonality and remainder are the three components of the STL plot. A box cox transformation was applied to the turnover to minimize the effect of seasonality on the remainder. The STL plot makes it easier to understand the graphical data at an individual level. The window for trend has been increased from default to get a smoother trend. The window for seasonality has also been increased to make it constant over time.

#### Differencing the transformed data

The differencing was done to the box-cox transformed data with a lag of 12, to remove trend and seasonality from the data. A second differencing was done to further remove any remaining patterns and further stabilize the data. 

```{r, warning=FALSE}
myseries %>%
  gg_tsdisplay((box_cox(Turnover,lambda = 0.1)%>% 
             difference(lag = 12)) %>%
               difference(), plot_type = "partial",lag_max  = 96)
```


```{r}

myseries %>%
  features(box_cox(Turnover, lambda = 0.1) %>% 
             difference(lag = 12) %>% 
             difference(),
           unitroot_kpss)
```

The KPSS test is used to assess the stationality of a time series. The low KPSS statistic value and the p-value greater than 0.05  suggest that the transformed and differenced series is stationary.


#### ARIMA Models

When constructing an ARIMA model we have to look at the significant spikes in the acf and pacf plots. The approach to constructing this model was simple to understand implement. 

For ARIMA1 : 

For p = 2, we have chosen the significant spike at 2 because after this most of the spikes or lags become insignificant.
For d = 1, we have chosen this because a differencing of 1 was applied to remove the seasonal component.
For q = 2, we have chosen the significant spike at 2 because after this most of the spikes or lags become insignificant.

For ARIMA2 : 

For p = 2, we have chosen the significant spike at 2 because after this most of the spikes or lags become insignificant.
For d = 1, we have chosen this because a differencing of 1 was applied to remove the seasonal component.

Here the approach was to construct only an AR model to further simplify the model.

For ARIMA3 :

For p = 1, we have chosen the significant spike at 1 because this was most significant spike on the pacf plot
For d = 1, we have chosen this because a differencing of 1 was applied to remove the seasonal component.
For q = 1, we have chosen the significant spike at 1 because this was the most significant spike on the acf plot.


All of these model includes a seasonal component with a seasonal AR order of 1, seasonal differencing order of 1, and seasonal MA order of 1.

```{r}
fit <- myseries %>% 
  filter(year(Month) <= "2016") %>% 
  model(
    ARIMA1 = ARIMA(box_cox(Turnover, lambda = 0.1) ~ 0 + pdq(2, 1, 2) + PDQ(1, 1, 1)),
    ARIMA2 = ARIMA(box_cox(Turnover, lambda = 0.1) ~ 0 + pdq(2, 1, 0) + PDQ(1, 1, 1)),
    ARIMA3 = ARIMA(box_cox(Turnover, lambda = 0.1) ~ 0 + pdq(2, 1, 3) + PDQ(1, 1, 1)),
    auto = ARIMA(box_cox(Turnover, lambda = 0.1))
  )

glance(fit)

fit_ARIMA_full <- myseries %>% 
  model( auto = ARIMA(box_cox(Turnover, lambda = 0.1)))
  
```

The model for ARIMA "auto" will give us the best forecast based on AICc .A lower AICc indicates a better balance between model fit and complexity. "auto" achieves a good fit to the data while considering the number of parameters in the model.

```{r}
fit %>% 
  select(auto) %>% 
  gg_tsresiduals()
```

From the above plot, for the ARIMA model "auto" we can observe the examine if there are any systematic patterns or trends remaining in the residuals. We can observe that there is no remaining trends in the data.

```{r}
fit %>% 
  select(auto) %>% 
  augment() %>% 
  features(.innov, ljung_box,lag = 12)
```

The p-value of 0.7449883 suggests that there is no strong evidence of autocorrelation in the residuals of the model. The residuals are considered to be independent, and the model is capturing the temporal dependencies adequately.

```{r}
fit %>% 
  forecast( h = "2 years") %>% 
  accuracy(myseries)
```

From the table , we can observe the accuracy of the models based on the RMSE. Here , we can observe the RMSE of "ARIMA3", but we will. disregard this because AICc helps in deciding a model that is simple and can capture the model complexity accurately compared to RMSE.

```{r}
fit %>% 
  forecast(h = "2 years") %>% 
  filter(.model == "auto") %>% 
  autoplot(myseries) +
  ylab(label = "Turnover(million($AUD))") +
  labs(title = "Forecasting through ARIMA auto model")
```
From the plot we can observe the prediction intervals of the data for the predicted forecast and it has been observed that the predicted forecast can be seen as accurate. 

#### ETS Models

The approach here was to construct simplest models possible. 

The AAN model has the additive trend component , additive error component with no seasonality component. 
The ANN model has the additive trend component , no error component with no seasonality component.
The AAM model has the additive trend component , additive error component with multiplicative seasonality component.
The AAA model has the additive trend component , additive error component with additive seasonality component.

```{r}

fit_ETS <- myseries %>%
  filter(year(Month) <= "2016") %>%
  model(
    AAN = ETS(box_cox(Turnover, lambda = 0.1) ~ error("A") + trend("A") + season("N")),
    ANN = ETS(box_cox(Turnover, lambda = 0.1) ~ error("A") + trend("N") + season("N")),
    AAM = ETS(box_cox(Turnover, lambda = 0.1 ) ~ error("A") + trend("A") + season("M")),
    AAA = ETS(box_cox(Turnover, lambda = 0.1 ) ~ error("A") + trend("A") + season("A")),
    auto = ETS(box_cox(Turnover, lambda = 0.1))
  )

glance(fit_ETS)

fit_ETS_full <- myseries %>% 
  model(AAM = ETS(box_cox(Turnover, lambda = 0.1 ) ~ error("A") + trend("A") + season("M")))
```

Based on the AICc values, the AAM model has the lowest AICc value of 260.9137. Therefore, the AAM model would be considered the best model choice based on AICc.

```{r}
fit_ETS %>% 
  forecast( h = "2 years") %>% 
  accuracy(myseries)
```
Here we can observe the RMSE values from the table above for all the ETS models and can observe that the "AAM" model, has the least RMSE value. It is inline with the model selection criterion based on our AICc values.  

```{r}
fit_ETS %>% 
  select(AAM) %>% 
  gg_tsresiduals()
```

For the ETS model "AAM" model we can observe that there is still significant amount of trend and patterns left in the data. 

```{r}
fit_ETS %>% 
  select(AAM) %>% 
  augment() %>% 
  features(.innov, ljung_box,lag = 12)
```

From the ljungbox-test conducted it can be observed that the p-value is less than 0.05 , therefore it can be seen that there is evidence of auto correlation in the residuals of the model.

```{r}
fit_ETS %>% 
  forecast( h = "2 years") %>% 
    filter(.model == "AAM") %>% 
  autoplot(myseries) +
  ylab(label = "Turnover(million($AUD))") +
  labs(title = "Forecasting through the ETS AAM model")
```

The forecasting done through the AAM model has been accurate but compared to the ARIMA  model the predictions have been less accurate as the predictions are higher than actual values but still lie in the prediction intervals.

#### Comparison of the models chosen

```{r}
rmse_AR3 <- fit %>% 
  select(ARIMA3) %>% 
  accuracy()

rmse_AAM <- fit_ETS %>% 
  select(AAM) %>% 
  accuracy()

rbind(rmse_AR3,rmse_AAM)
```

On comparing the RMSE of of the chosen ARIMA and ETS models , we can compare the RMSE of the two models to get a better insight on the accuracy and predictions of the data. Here we can see that ARIMA3 model performs better compared to AAM model as the RMSE of ARIMA3 is lower. 

#### Out of Sample Plots for ARIMA and ETS Models

```{r}
fit_ARIMA_full %>% 
  forecast(h = "2 years") %>% 
  autoplot(myseries)  +
  ylab(label = "Turnover(million($AUD))") +
  labs(title = "Out of sample forecast for ARIMA auto model ")
```


```{r}
fit_ETS_full %>% 
  forecast(h = "2 years") %>% 
  autoplot(myseries)  +
  ylab(label = "Turnover(million($AUD))") +
  labs(title = "Out of sample forecast for ETS AAM model ")
```


#### Comparison with the new data

```{r warning=FALSE, message=FALSE}

fit_ETS_forecast <- fit_ETS %>% 
  forecast(h = "2 years") %>% 
  filter(.model == "AAM")

fit_ar_forecast <- fit %>% 
  forecast(h = "2 years") %>% 
  filter(.model == "auto")

url <- paste0("https://www.abs.gov.au/statistics/industry/retail-and-wholesale-trade/retail-trade-australia/feb-2023/8501011.xlsx")

tab11 <- read_abs_url(url) %>% 
  filter(series_id == "A3349410F") %>% 
  filter(year(date) >= "2019" & year(date) <= "2020")

AR_final <-fit_ar_forecast %>% 
  bind_cols(as.numeric(fit_ar_forecast$.mean),as.numeric(tab11$value)) %>% 
  as.data.frame()

ETS_final <-fit_ETS_forecast %>% 
  bind_cols(as.numeric(fit_ETS_forecast$.mean),as.numeric(tab11$value)) %>% 
  as.data.frame()

rmse_AR <- sqrt(mean((AR_final$...8 - AR_final$...7)^2))
rmse_ETS <- sqrt(mean((ETS_final$...8 - ETS_final$...7)^2))

rmse_AR
rmse_ETS
```

When we compare the predictions of the model with the actual values of the data,  it can be observed that the ARIMA model performs lower in expectations compared to ETS model. 

The ETS model is able to capture the complexities of the model more accurately compared to ARIMA model is significantly more accurate in predicting the accuracy of the model. 

#### Discussion on the benefits and limitations of the Models 

Benefits :

- Both models are accurate in interpretability of the data and can capture the underlying trends and seasonality of the data very well. 
- The model selection done by ETS is done automatically which simplifies the modelling process.
- ARIMA models are more flexible as they can capture trends, seasonality, and irregularities as well as handle a wide range of time series data that is stationary or non-stationary.
- ETS models are designed in such a way that they can accommodate various patterns, including additive and multiplicative trends and seasonality.
- They are both effective for forecasting long term data and ETS is very accurate in capturing the long term trends of the data.

Limitations :

- In the ARIMA model we need to have the data stationary or make  non-stationary data into stationary. 
- In the ARIMA model we need to select appropriate p,d,q values which leads to trial and error and can lead to sub-par models.
- The ETS model does not handle trends very well.
- In the ETS model the selection of parameters are very complex. 

